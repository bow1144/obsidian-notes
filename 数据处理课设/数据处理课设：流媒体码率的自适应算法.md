- 姓名：张博文
- 班级：大数据2204
- 学号：8208220420
---
## 一、实验概括

### 1.1 实验目标

学习流媒体视频传输的基本原理和数据处理方法，通过 Python 编程语言和相关库分析网络流量数据，理解并优化流媒体传输中的码率自适应算法，重点提高以下能力：
- 掌握网络带宽对码率的影响分析
- 提高视频播放的流畅性和用户体验
- 掌握 Python 工具及算法设计

### 1.2 实验内容

1. **实验环境学习**：完成实验平台环境配置，阅读 SIM 和 ABR 部分算法的程序，学习实验平台的基础知识
2. **网络带宽数据分析**：对给定网络流量数据进行处理和可视化，尝试预测带宽变化
3. **仿真程序运行与分析**：利用仿真平台模拟视频播放过程，评估默认的 BBA 算法的优缺点和性能指标
4. **设计新的自适应算法**：根据仿真器的输出数据，设计新的码率自适应算法代替 BBA ，通过仿真平台验证算法性能
5. **实际环境下测试算法**：分析网络带宽波动和视频类型对码率自适应算法性能的影响

### 1.3 实验开发环境

- Windows 11 操作系统
- python 3.8
- Anaconda 3
- Pycharm
- LiveStreamingEnv 实验平台

### 1.4 关键理论

- **流媒体视频播放过程**
	- 视频从up主端实时上传到**转码**服务器，转码为不同码率
	- 用户客户端根据网络情况从 CDN 节点请求码率合适的视频流
- **码率的自适应算法**
	- 基于网络带宽和缓冲区状态，动态选择码率优化客户体验
- **算法的性能指标**
	- 视频卡顿时间：越低越好
	- 码率等级：越高越好
	- CDN 与播放端的延时：越低越好
	- 码率切换频率：减少码率切换次数和跨度

### 1.5 实验任务

1. 分析网络带宽数据，尝试预测变化趋势
2. 使用默认的 BBA 算法运行仿真程序
3. 设计新算法，优化码率自适应策略
4. 比较不同的网络环境和带宽对算法性能的影响

### 1.6 评价指标

通过用户体验质量评价算法的优劣
- 奖励项：码率
- 惩罚项：卡顿时间，延时，码率切换频次与跨度

---
## 二、实验环境配置与学习

### 2.1 仿真平台框架结构

#### 2.1.1 SIM 部分

- **功能**：
	- 模拟视频块下载和播放的过程
	- 根据带宽、延迟等网络环境变化，生成视频播放相关的性能参数
- **输入**：
	- **视频数据**：
		- 视频帧的**时间戳、数据大小、帧类型**
		- 视频帧按照 GOP 组织， 每个视频块由多个帧组成，但有且仅有一个**I帧**
	- **网络数据**：
		- 以 500ms 为采样频率生成的带宽变化情况
		- 描述网络带宽随时间动态变化
- **输出**：
	- 每次视频快下载完成后，生成一组仿真数据：
		- 下载的数据量、缓冲区大小、卡顿时间、延时等指标
		- 是否需要进行码率决策

#### 2.1.2 ABR 部分

- **功能**：
    - 根据 SIM 部分生成的网络和播放参数，选择下一段视频块的下载码率
    - 动态调整下载策略以优化用户体验
- **输入**：
    - SIM 部分生成的实时网络和播放状态数据
    - 用户设置的目标缓冲区大小等。
- **输出**：
    - 决策码率
    - 将码率选择结果返回给 SIM 模块，继续模拟视频块的下载和播放。

### 2.2 仿真平台的核心逻辑

#### 2.2.1 视频下载与播放

- 客户端从 CDN 下载视频帧并存入缓冲区。
- 客户端播放缓冲区中的视频帧，计算缓冲区剩余大小
- 如果缓冲区为零，则产生卡顿
- 当下载一个完整的 GOP（下一个 I 帧）时，触发码率选择

仿真器返回的关键数据：
- **物理时间相关参数**：
	- `time`：当前时间
	- `time_interval`：仿真时间周期长度
- **下载与缓冲相关参数**：
	- `send_data_size`：本周期下载的数据量
	- `buffer_size`：缓冲区大小
	- `chunk_len`：当前视频块的播放时长
- **性能指标**：
	- `rebuf`：卡顿时间
	- `end_delay`：端到端延时
- **状态标志**：
	- `decision_flag`：是否出发码率决策
	- `buffer_flag`：播放器是否处于缓冲状态
	- `end_of_video`：视频是否放完

#### 2.2.2 码率自适应算法

- 基于**当前缓冲区、网络带宽、延时卡顿情况**选择码率
- 调整码率平衡以下指标：**高码率、低延时、减少码率切换频率**

#### 2.2.3 仿真器的优化策略

- **跳帧**：当延时超过阈值时，跳过部分帧以减少延时
- **快慢播**：根据缓冲区剩余大小，加快或减慢播放速度

### 2.3 视频和网络组织模式

#### 2.3.1 视频数据格式

- 视频流由多个 GOP 视频块组成，**每个GOP包含**
	- **I帧**：唯一的关键帧，用于码率切换的基础
	- **P帧和B帧**：依赖于前后帧的预测帧
- **视频帧的数据格式**：
	- **时间戳（秒）**：帧到达 CDN 服务器的时间。
	- **帧数据大小（bit）**：该帧的大小。
	- **帧类型（0或1）**：是否为 I 帧。

#### 2.3.2 网络数据格式

- **时间戳（秒）**：网络状态采样的时间点。
- **吞吐量（Mbps）**：当前时刻的网络吞吐量。

----
## 三、online 仿真程序分析

### 3.1 网络与视频数据加载

```
all_cooked_time, all_cooked_bw, all_file_names = load_trace.load_trace(TRAIN_TRACES)
```
- 使用`load_trace`模块加载网络带宽数据，返回以下内容：
	- `all_cooked_time`：网络采样时间戳
	- `all_cooked_bw`：每个时间戳的带宽
	- `all_file_names`：网络数据的文件名列表

### 3.2 仿真环境初始化

```
net_env = fixed_env.Environment(
    all_cooked_time=all_cooked_time,
    all_cooked_bw=all_cooked_bw,
    random_seed=random_seed,
    logfile_path=LogFile_Path,
    VIDEO_SIZE_FILE=video_size_file,
    Debug=DEBUG
)
```

### 3.3 码率自适应算法和状态变量初始化

```
abr = ABR.Algorithm()
abr_init = abr.Initial()

BIT_RATE = [500.0, 850.0, 1200.0, 1850.0]  # kpbs
TARGET_BUFFER = [2.0, 3.0]  # seconds
RESEVOIR = 0.5
CUSHION = 2
```
- 客户端支持四种码率`[500.0, 850.0, 1200.0, 1850.0]`，单位kpbs
- 支持两种缓冲区大小`[2.0, 3.0]`，单位秒
- **`RESEVOIR`** 和 **`CUSHION`** 定义了缓冲区调节的规则

### 3.4 主循环中调用仿真环境获取状态

```
time, time_interval, send_data_size, chunk_len, rebuf, buffer_size, play_time_len, end_delay, \
cdn_newest_id, download_id, cdn_has_frame, decision_flag, buffer_flag, cdn_flag, end_of_video = \
net_env.get_video_frame(bit_rate, target_buffer)
```
仿真环境返回当前帧的下载和播放状态，包括：
- **时间相关**：`time`, `time_interval`
- **传输状态**：`send_data_size`, `chunk_len`, `rebuf`
- **缓冲区状态**：`buffer_size`, `buffer_flag`
- **延迟相关**：`end_delay`, `cdn_flag`

```
S_time_interval.pop(0)  
S_send_data_size.pop(0)  
S_chunk_len.pop(0)  
S_buffer_size.pop(0)  
S_rebuf.pop(0)  
S_end_delay.pop(0)  
S_play_time_len.pop(0)  
S_decision_flag.pop(0)  
S_buffer_flag.pop(0)  
S_cdn_flag.pop(0)  
  
S_time_interval.append(time_interval)  
S_send_data_size.append(send_data_size)  
S_chunk_len.append(chunk_len)  
S_buffer_size.append(buffer_size)  
S_rebuf.append(rebuf)  
S_end_delay.append(end_delay)  
S_play_time_len.append(play_time_len)  
S_decision_flag.append(decision_flag)  
S_buffer_flag.append(buffer_flag)  
S_cdn_flag.append(cdn_flag)
```
更新状态序列

### 3.5 主循环中 QoE 的计算

```
if not cdn_flag:
    reward_frame = frame_time_len * float(BIT_RATE[bit_rate]) / 1000 - REBUF_PENALTY * rebuf - LANTENCY_PENALTY * end_delay
else:
    reward_frame = -(REBUF_PENALTY * rebuf)

reward_frame += -1 * SMOOTH_PENALTY * (abs(BIT_RATE[bit_rate] - BIT_RATE[last_bit_rate]) / 1000)
```
- Frame QoE 计算：
$$QoE_1 = play_{time} * bitRate - 1.5 * rebuf - 0.005 * latency$$
- Chunk QoE 计算：
$$ QoE_2 = -0.02 * \frac{|bitRate - lastBitRate|}{1000}$$
- 实际 QoE 为二者之和：
$$ QoE = QoE_1 + QoE_2 $$

### 3.6 码率和缓冲决策

```
bit_rate, target_buffer = abr.run(
    time, S_time_interval, S_send_data_size, S_chunk_len, S_rebuf,
    S_buffer_size, S_play_time_len, S_end_delay, S_decision_flag,
    S_buffer_flag, S_cdn_flag, end_of_video, cdn_newest_id, download_id,
    cdn_has_frame, abr_init
)
```
- 根据状态变量动态决策码率（`bit_rate`）和目标缓冲区（`target_buffer`）

### 3.7 视频结束处理

```
if end_of_video:
    video_count += 1
    if video_count >= len(all_file_names):
        break
```

----
## 四、网络带宽数据分析与预测

### 4.1 数据预处理与可视化

- 描述性统计一段网络带宽变化
```
def basic(data):  
    mean = np.mean(data)  
    sd = np.var(data)  
    print("均值："+str(mean))  
    print("方差："+str(sd))  
    return mean
```
- 得出均值为`1.198`，方差为`0.21`
* 网络带宽数据是一种**单变量的时间序列**数据
- 将数据从`network_trace/0`中提取到`numpy`数组中
- 绘制折线图，尝试可视化数据
![[{5F798181-6BD7-4B5F-B239-75BBDEC18D40}.png]]

- 前50个数据
![[{5866B044-AFD8-45E3-972C-118FA518B92C}.png]]
- 可以看到网络带宽数据从肉眼观察下并没有显著的规律，尝试使用统计规律

- 网络数据的分布如下，可以观察到数据呈现右偏分布（右尾较长）。
- 主体数据集中在靠近 0 到 2 的区域，分布的密度逐渐减小。
![[{1CC94B98-B71C-4D62-9319-4BEEEEA40C03}.png]]

### 4.2 数据规律挖掘

#### 4.2.1 趋势挖掘

* 首先考虑数据的总体趋势，虽然考虑到本数据集没有开始和结束相关信息，总体趋势很难挖掘出规律，但还是先试试
* 分别用15，30，60，120的窗口平滑处理数据，方便观察趋势（下图以`window_size = 30`为例	
```
window_size = 30  
moving_avg = np.convolve(data, np.ones(window_size)/window_size, mode='valid')
```
 ![[{F27F37DD-6476-4767-9CD2-EE9728A4B922}.png]]
- 显然，**数据没有显著的趋势**

#### 4.2.2 周期挖掘

- 考虑到网络流量的变化可能存在周期规律，本实验使用自相关系数（ACF）和偏自相关系数（PACF）挖掘数据的周期性
```
fig, ax = plt.subplots(1, 2, figsize=(15, 6))  
plot_acf(data, lags=250, ax=ax[0])  # 自相关图  
plot_pacf(data, lags=250, ax=ax[1])  # 偏自相关图  
plt.show()
```
![[{5DD8DBC6-16BD-4C12-AF62-71D8D1B97E11}.png]]
- ACF 和 PACF 系数均没有大于 0.1 的点，故可以认为数据几乎没有周期性

#### 4.2.3 白噪声判断

- 使用 Ljung-Box 法判断带宽数据是否为以均值为中心的白噪声
```
def is_noise(data):  
    data = data - basic(data)  
    result = acorr_ljungbox(data, lags=[10], return_df=True)  
    return result[1]
```
- `p_value = 0.616` ，p_value>0.05，不能拒绝原假设，可以认为带宽数据是以平均值为中心的白噪声

## 五、BBA 算法尝试与分析

### 5.1 BBA 算法的简介

**码率自适应算法 BBA (Buffer-Based Adaptive Algorithm)** 是一种在视频流传输中使用的自适应比特率算法，其主要目标是根据用户设备的缓冲区状态动态调整视频的比特率，以在视频质量和播放稳定性之间实现平衡。与传统基于带宽预测的自适应算法不同，BBA更注重缓冲区的状态

BBA 是基于缓冲区的自适应算法，它假设缓冲区状态能够反映网络条件以及播放稳定性，算法主要依据当前缓冲区的填充水平决定选择的视频质量（比特率）

1. **缓冲区驱动决策**
    - 如果缓冲区较为充足，说明网络状况较好或者有足够的播放缓存支持，可以选择更高的比特率以提升视频质量
    - 如果缓冲区较低，说明网络状况可能较差，或者存在潜在的缓冲区耗尽风险，此时选择较低的比特率以避免视频卡顿
2. **目标：平衡质量与稳定性**
    - **避免卡顿**：通过监测缓冲区，优先保证播放流畅性
    - **优化质量**：在播放流畅性得以保障的前提下，提高视频的比特率以改善画质

### 5.2 仿真程序中的 BBA 算法

#### 5.2.1 核心参数

- `RESEVOIR`（缓冲池）：最小缓冲区阈值。如果缓冲区小于这个值，算法选择最低比特率（`bit_rate = 0`）
- `CUSHION`（缓冲余量）：表示在缓冲区足够大时，可以提升比特率
```
RESEVOIR = 0.4
CUSHION = 1

```
#### 5.2.2 缓冲区驱动的码率决策

```
if S_buffer_size[-1] < RESEVOIR:
    bit_rate = 0
elif S_buffer_size[-1] >= RESEVOIR + CUSHION:
    bit_rate = 2
elif S_buffer_size[-1] >= CUSHION + CUSHION:
    bit_rate = 3
else:
    bit_rate = 1
```
- 如果当前缓冲池大小小于**最小阈值**，那么选择**最低码率**
- 如果缓冲池有**足够余量**，选择二档码率
- 如果有**两倍余量**，选择最高码率
- 其余情况选择第一档码率

### 5.3 运行仿真程序

- 使用`pytest`运行`online`仿真程序，关闭 DEBUG 模式，运行时间为 432.3s ，结果如下

![[{364C95F3-19FB-4E7D-AF3C-311DD4F530C7}.png]]

### 5.4 BBA 算法的优缺点

#### 5.4.1 BBA 算法的优点

- **自适应性强**：能够实时根据网络调整码率，各种带宽环境下都能提供服务
- **避免卡顿**：可以使用降低码率应对卡顿
- **实现简单**：相比于复杂的码率适应算法，BBA 算法的实现非常简单

#### 5.4.2 BBA 算法的缺点

- **延时问题**：需要实时检测网络进行调整，网络波动会造成延时
- **快慢播影响视频质量**：使用快慢播适配网络，降低用户体验
- **难以预测网络状况**：网络带宽的波动有时很难准确预测，特别是在用户网络环境复杂多变的情况下，BBA算法的自适应过程可能会受到限制

## 六、新算法的设计

相比于简单的 BBA 算法，由易到难地设计新算法

### 6.1 基于 BBA 的简单调参【41.61】

- 调整缓存策略的阈值，`RESEVOIR = 0.6  CUSHION = 0.8`
![[{48C7C640-4037-4FFF-ADBF-841D2E882B1B}.png]]
- 可以看到，分数虽有提高，但提高程度不大，需要考虑更多变量，训练复杂的模型
![[{827436F1-1ED1-41ED-875E-8405463F2BE0}.png]]

### 6.2 简单的随机森林模型【35.76】

- 将数据分为测试集和训练集，在训练集中使用`S_time_interval[-1], S_send_data_size[-1], S_chunk_len[-1],  S_rebuf[-1], S_buffer_size[-1], S_end_delay[-1], S_play_time_len[-1]]`变量进行随机森林训练
- 训练集大小：14790
```
X_train = list(X_train)  
y_train_bitrate = list(y_train_bitrate)  
y_train_buffer = list(y_train_buffer)  
X_train.append([  
    S_time_interval[-1], S_send_data_size[-1], S_chunk_len[-1],  
    S_rebuf[-1], S_buffer_size[-1], S_end_delay[-1], S_play_time_len[-1]  
])  
y_train_bitrate.append(bit_rate)  
y_train_buffer.append(target_buffer)
```

```
bitrate_model = RandomForestRegressor(n_estimators=100, random_state=42)  
buffer_model = RandomForestRegressor(n_estimators=100, random_state=42)
```
- 第一次训练：训练周期小，计算时间极长，但训练效果其实并不好
![[{4C9CE716-2473-45FB-8FA4-41B14F4D6ADC}.png]]

* 随后替换成了简单的线性回归模型，效果更差

* 事实证明，由于**没有正确答案**，该模型应该属于无监督学习模型，而且普通的无监督学习，例如聚类在本问题中并没有很好的解决方案，故采取**强化学习**模型解决问题

### 6.3 强化学习的概念

强化学习（RL）是一种基于试错的学习方式，智能体通过与环境的交互，学习如何在给定的环境中采取动作，以最大化长期的累计奖励。与监督学习不同，强化学习的目标不是学习输入与输出之间的映射关系，而是学习如何根据当前状态选择合适的动作，从而获得最大的奖励。

强化学习的目标是通过训练智能体使其学会一个最优策略 $π^∗(s)$，该策略能够使得智能体在所有可能的状态下最大化累积奖励。这可以通过 **价值迭代** 或 **策略迭代** 来实现。

在ABR算法中，强化学习的核心组成部分可以映射为：
- **智能体（Agent）**：视频播放器或客户端。智能体负责根据当前状态选择最合适的比特率。
- **环境（Environment）**：网络环境，包括带宽、延迟、网络波动等因素。环境提供反馈信息，如网络带宽、缓存状态、延迟等。
- **状态（State, $s_t$​)**：状态可以包括当前的带宽、缓存大小、视频质量、当前帧大小、延迟、丢包率等。
- **动作（Action, $a_t$)**：在每个时刻，智能体选择一个动作，即调整视频流的比特率。比如，可以选择低、中、高三个比特率级别，或者根据不同编码的比特率值进行微调。
- **奖励（Reward, $r_t$​)**：奖励通常由以下几个因素构成：
    - **码率**
    - **卡顿时间**
    - **切换频次**
    - **延迟**
    奖励函数通常结合这些因素，赋予智能体选择合理比特率的目标。
- **策略（Policy, $\pi$)**：策略是智能体选择动作的规则，通常是从当前状态 $s_t$​ 选择动作 $a_t$ 。强化学习算法的目标是学习一个最优策略，选择在当前状态下能获得最大累计奖励的比特率。

### 6.4 强化学习在 ABR 算法中的应用

#### 6.4.1 数据预处理

* 共设置七个状态：`S_time_interval, S_send_data_size, S_chunk_len,  S_rebuf, S_buffer_size, S_end_delay, S_play_time_len`
* 数据预处理：
```
df = pd.DataFrame({  
    'op0' : X_train[:,0],  
    'op1' : X_train[:,1],  
    'op2' : X_train[:,2],  
    'op3' : X_train[:,3],  
    'op4' : X_train[:,4],  
    'op5' : X_train[:,5],  
    'op6' : X_train[:,6],  
    'Reward' : reward_list,  
    'BitRate' : y_train_bitrate,  
    'TargetBuffer' : y_train_buffer  
})
```

#### 6.4.2 构建强化学习的 DQN 网络

```
class DQNAgent:  
    def __init__(self, state_dim, action_dim):  
        self.state_dim = state_dim  
        self.action_dim = action_dim  
        self.memory = deque(maxlen=10000)  
        self.gamma = 0.99  # 折扣因子  
        self.epsilon = 1.0  # 探索率  
        self.epsilon_min = 0.01  
        self.epsilon_decay = 0.995  
        self.batch_size = 32  
        self.learning_rate = 0.001  
  
        # Q 网络  
        self.q_network = self.build_network()  
        self.target_network = self.build_network()  
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)  
  
    def build_network(self):  
        """构建 Q 网络"""  
        return nn.Sequential(  
            nn.Linear(self.state_dim, 16),  
            nn.ReLU(),  
            nn.Linear(16, 12),  
            nn.ReLU(),  
            nn.Linear(12, self.action_dim)  
        )  
  
    def select_action(self, state):  
        """选择动作：ε-greedy 策略"""  
        if np.random.rand() < self.epsilon:  
            return random.randrange(self.action_dim)  # 随机选择动作（探索）  
        else:  
            state_tensor = torch.FloatTensor(state).unsqueeze(0)  
            q_values = self.q_network(state_tensor)  
            return torch.argmax(q_values).item()  # 选择 Q 值最大对应的动作  
  
    def store_experience(self, state, action, reward, next_state, done):  
        """存储经验到经验池"""  
        self.memory.append((state, action, reward, next_state, done))  
  
    def train(self):  
        """训练 Q 网络"""  
        if len(self.memory) < self.batch_size:  
            return  
  
        batch = random.sample(self.memory, self.batch_size)  
        states, actions, rewards, next_states, dones = zip(*batch)  
  
        states = torch.FloatTensor(states)  
        actions = torch.LongTensor(actions)  
        rewards = torch.FloatTensor(rewards)  
        next_states = torch.FloatTensor(next_states)  
        dones = torch.BoolTensor(dones)  
  
        # 计算 Q 值  
        q_values = self.q_network(states)  
        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  
  
        # 计算目标 Q 值  
        with torch.no_grad():  
            target_q_values = self.target_network(next_states)  
            max_next_q_values = torch.max(target_q_values, dim=1)[0]  
            q_target = rewards + (self.gamma * max_next_q_values * (~dones))  
  
        # 计算损失并更新网络  
        loss = nn.MSELoss()(q_value, q_target)  
        self.optimizer.zero_grad()  
        loss.backward()  
        self.optimizer.step()  
  
        # 更新目标网络  
        self.update_target_network()  
  
    def update_target_network(self):  
        """每隔一定的步数更新目标网络"""  
        self.target_network.load_state_dict(self.q_network.state_dict())  
  
    def decay_epsilon(self):  
        """逐渐减少探索率"""  
        if self.epsilon > self.epsilon_min:  
            self.epsilon *= self.epsilon_decay
```

#### 6.4.3  载入数据

```
# 创建 DQN 智能体并训练  
state_dim = 7  # 假设有 7 个自变量  
action_dim = 4  # 4 种码率和缓存配置（例如：低码率、中码率、高码率等）  
  
agent = DQNAgent(state_dim=state_dim, action_dim=action_dim)  
data = pd.read_csv("train.csv")
```

#### 6.4.4 RL训练

```
print("start train")  
# 训练过程中与环境交互（这里只是示例，需要你根据实际情况调整）  
for episode in range(len(data)):  
    state = data.iloc[episode, :7].values  # 通过 iloc 索引来访问数据  
    done = False  
    total_reward = 0  
  
    while not done:  
        action = agent.select_action(state)  # 选择动作（码率和缓存）  
        next_state = np.random.rand(state_dim)  # 下一个状态，假设随机生成  
        reward = data.iloc[episode, 9]  # 奖励，假设随机生成  
        done = np.random.rand() > 0.9  # 假设有 10% 的概率结束  
        agent.store_experience(state, action, reward, next_state, done)  
        agent.train()  # 训练 Q 网络  
        agent.decay_epsilon()  # 更新探索率  
  
        state = next_state  
        total_reward += reward  
  
    print(f"Episode {episode}, Total Reward: {total_reward}")
```
* 训练过程如下，训练时间约 24min
![[{E5A039D8-6FD8-4088-B58F-6026022CF494}.png]]

#### 6.4.5 将训练结果载入测试单元

- 结果如下，得分为 42.49，略有提升
![[{D7A160E5-C8BB-447A-8AEC-C207719E0E4F}.png]]
